ccopy_reg
_reconstructor
p1
(c__main__
MLP
p2
c__builtin__
object
p3
NtRp4
(dp5
S'L2_sqr'
p6
g1
(ctheano.tensor.var
TensorVariable
p7
g3
NtRp8
(dp9
S'auto_name'
p10
S'auto_56'
p11
sS'index'
p12
I0
sS'tag'
p13
(itheano.gof.utils
scratchpad
p14
(dp15
S'trace'
p16
(lp17
(lp18
(S'pickletest.py'
p19
I110
S'<module>'
p20
S'test()'
tp21
a(S'pickletest.py'
p22
I105
S'test'
p23
S'MLPtest=MLP(rng=numpy.random.RandomState(1234),input=numpy.ndarray([2,3]),n_in=2, n_hidden=3, n_out=4)'
tp24
a(S'pickletest.py'
p25
I89
S'__init__'
p26
S'+ (self.logRegressionLayer.W ** 2).sum()'
tp27
aasbsS'name'
p28
NsS'owner'
p29
g1
(ctheano.gof.graph
Apply
p30
g3
NtRp31
(dp32
S'inputs'
p33
(lp34
g1
(g7
g3
NtRp35
(dp36
g10
S'auto_45'
p37
sg12
I0
sg13
(itheano.gof.utils
scratchpad
p38
(dp39
g16
(lp40
(lp41
(g19
I110
g20
S'test()'
tp42
a(g22
I105
g23
S'MLPtest=MLP(rng=numpy.random.RandomState(1234),input=numpy.ndarray([2,3]),n_in=2, n_hidden=3, n_out=4)'
tp43
a(g25
I88
g26
S'(self.hiddenLayer.W ** 2).sum()'
tp44
aasbsg28
Nsg29
g1
(g30
g3
NtRp45
(dp46
g33
(lp47
g1
(g7
g3
NtRp48
(dp49
g10
S'auto_44'
p50
sg12
I0
sg13
(itheano.gof.utils
scratchpad
p51
(dp52
g16
(lp53
(lp54
(g19
I110
g20
S'test()'
tp55
a(g22
I105
g23
S'MLPtest=MLP(rng=numpy.random.RandomState(1234),input=numpy.ndarray([2,3]),n_in=2, n_hidden=3, n_out=4)'
tp56
a(g25
I88
g26
S'(self.hiddenLayer.W ** 2).sum()'
tp57
aasbsg28
Nsg29
g1
(g30
g3
NtRp58
(dp59
g33
(lp60
g1
(ctheano.tensor.sharedvar
TensorSharedVariable
p61
g3
NtRp62
(dp63
g10
S'auto_3'
p64
sg12
Nsg13
(itheano.gof.utils
scratchpad
p65
(dp66
g16
(lp67
(lp68
(g19
I110
g20
S'test()'
tp69
a(g22
I105
g23
S'MLPtest=MLP(rng=numpy.random.RandomState(1234),input=numpy.ndarray([2,3]),n_in=2, n_hidden=3, n_out=4)'
tp70
a(g25
I73
g26
S'activation=T.tanh'
tp71
a(S'pickletest.py'
p72
I47
g26
S"W = theano.shared(value=W_values, name='W', borrow=True)"
tp73
aasbsS'container'
p74
g1
(ctheano.gof.link
Container
p75
g3
NtRp76
(dp77
g28
S'W'
sS'storage'
p78
(lp79
cnumpy.core.multiarray
_reconstruct
p80
(cnumpy
ndarray
p81
(I0
tS'b'
tRp82
(I1
(I2
I3
tcnumpy
dtype
p83
(S'f8'
I0
I1
tRp84
(I3
S'<'
NNNI-1
I-1
I0
tbI00
S'zQ>\xef\x89\xa0\xe5\xbf\xb4\x18\n4)\x1f\xd1?pV\x80\xf3\x97v\xc1\xbf>\xa3p\x0f\x8d\x01\xe4?d3\xf22\xf1\xa0\xe3?0h\xfc\x98\xe9\xe2\xdf\xbf'
tbasS'strict'
p85
I00
sS'readonly'
p86
I00
sS'type'
p87
g1
(ctheano.tensor.type
TensorType
p88
g3
NtRp89
(dp90
S'broadcastable'
p91
(I00
I00
tp92
sS'dtype'
p93
S'float64'
p94
sS'numpy_dtype'
p95
g84
sS'sparse_grad'
p96
I00
sg28
NsbsS'allow_downcast'
p97
Nsbsg28
S'W'
sg29
Nsg87
g89
sbag1
(g7
g3
NtRp98
(dp99
g10
S'auto_43'
p100
sg12
I0
sg13
(itheano.gof.utils
scratchpad
p101
(dp102
g16
(lp103
(lp104
(g19
I110
g20
S'test()'
tp105
a(g22
I105
g23
S'MLPtest=MLP(rng=numpy.random.RandomState(1234),input=numpy.ndarray([2,3]),n_in=2, n_hidden=3, n_out=4)'
tp106
a(g25
I88
g26
S'(self.hiddenLayer.W ** 2).sum()'
tp107
aasbsg28
Nsg29
g1
(g30
g3
NtRp108
(dp109
g33
(lp110
g1
(ctheano.tensor.var
TensorConstant
p111
g3
NtRp112
(dp113
g10
S'auto_39'
p114
sg12
Nsg13
(itheano.gof.utils
scratchpad
p115
(dp116
S'unique_value'
p117
Nsbsg28
NsS'cached'
p118
I01
sg87
g1
(g88
g3
NtRp119
(dp120
g91
(tsg93
S'int8'
p121
sg95
g83
(S'i1'
I0
I1
tRp122
(I3
S'|'
NNNI-1
I-1
I0
tbsg96
I00
sg28
NsbsS'data'
p123
g80
(g81
(I0
tS'b'
tRp124
(I1
(tg122
I00
S'\x02'
tbsbasg13
(itheano.gof.utils
scratchpad
p125
(dp126
bsS'outputs'
p127
(lp128
g98
asS'op'
p129
g1
(ctheano.tensor.elemwise
DimShuffle
p130
g3
NtRp131
(dp132
S'drop'
p133
(lp134
sS'shuffle'
p135
(lp136
sS'augment'
p137
(lp138
I0
aI1
asS'input_broadcastable'
p139
(tsS'inplace'
p140
I00
sS'new_order'
p141
(S'x'
S'x'
tp142
sS'_op_use_c_code'
p143
S'/usr/bin/g++'
p144
sbsbsg87
g1
(g88
g3
NtRp145
(dp146
g91
(I01
I01
tp147
sg93
g121
sg95
g122
sg96
I00
sg28
Nsbsbasg13
(itheano.gof.utils
scratchpad
p148
(dp149
bsg127
(lp150
g48
asg129
g1
(ctheano.tensor.elemwise
Elemwise
p151
g3
NtRp152
(dp153
S'__module__'
p154
S'tensor'
p155
sS'scalar_op'
p156
g1
(ctheano.scalar.basic
Pow
p157
g3
NtRp158
(dp159
S'output_types_preference'
p160
ctheano.scalar.basic
upcast_out
p161
sg143
g144
sg28
S'pow'
p162
sbsg28
S'Elemwise{pow,no_inplace}'
p163
sg143
g144
sS'destroy_map'
p164
(dp165
sS'nfunc_spec'
p166
(S'power'
p167
I2
I1
tp168
sS'inplace_pattern'
p169
(dp170
sS'openmp'
p171
I00
sS'__doc__'
p172
S"elementwise power\n\n    Generalizes a scalar op to tensors.\n\n    All the inputs must have the same number of dimensions. When the\n    Op is performed, for each dimension, each input's size for that\n    dimension must be the same. As a special case, it can also be 1\n    but only if the input's broadcastable flag is True for that\n    dimension. In that case, the tensor is (virtually) replicated\n    along that dimension to match the size of the others.\n\n    The dtypes of the outputs mirror those of the scalar Op that is\n    being generalized to tensors. In particular, if the calculations\n    for an output are done inplace on an input, the output type must\n    be the same as the corresponding input type (see the doc of\n    scalar.ScalarOp to get help about controlling the output type)\n\n    Parameters\n    ----------\n    scalar_op\n        An instance of a subclass of scalar.ScalarOp which works uniquely\n        on scalars.\n    inplace_pattern\n        A dictionary that maps the index of an output to the\n        index of an input so the output is calculated inplace using\n        the input's storage. (Just like destroymap, but without the lists.)\n    nfunc_spec\n        Either None or a tuple of three elements,\n        (nfunc_name, nin, nout) such that getattr(numpy, nfunc_name)\n        implements this operation, takes nin inputs and nout outputs.\n        Note that nin cannot always be inferred from the scalar op's\n        own nin field because that value is sometimes 0 (meaning a\n        variable number of inputs), whereas the numpy function may\n        not have varargs.\n\n    Examples\n    --------\n    Elemwise(add) # represents + on tensors (x + y)\n    Elemwise(add, {0 : 0}) # represents the += operation (x += y)\n    Elemwise(add, {0 : 1}) # represents += on the second argument (y += x)\n    Elemwise(mul)(rand(10, 5), rand(1, 5)) # the second input is completed\n    # along the first dimension to match the first input\n    Elemwise(true_div)(rand(10, 5), rand(10, 1)) # same but along the\n    # second dimension\n    Elemwise(int_div)(rand(1, 5), rand(10, 1)) # the output has size (10, 5)\n    Elemwise(log)(rand(3, 4, 5))\n\n    "
p173
sbsbsg87
g1
(g88
g3
NtRp174
(dp175
g91
(I00
I00
tp176
sg93
S'float64'
p177
sg95
g84
sg96
I00
sg28
Nsbsbasg13
(itheano.gof.utils
scratchpad
p178
(dp179
bsg127
(lp180
g35
asg129
g1
(ctheano.tensor.elemwise
Sum
p181
g3
NtRp182
(dp183
S'acc_dtype'
p184
g177
sg93
g177
sg143
g144
sg156
g1
(ctheano.scalar.basic
Add
p185
g3
NtRp186
(dp187
g160
g161
sg143
g144
sg28
S'add'
p188
sbsS'axis'
p189
Nsbsbsg87
g1
(g88
g3
NtRp190
(dp191
g91
(tsg93
g177
sg95
g84
sg96
I00
sg28
Nsbsbag1
(g7
g3
NtRp192
(dp193
g10
S'auto_52'
p194
sg12
I0
sg13
(itheano.gof.utils
scratchpad
p195
(dp196
g16
(lp197
(lp198
(g19
I110
g20
S'test()'
tp199
a(g22
I105
g23
S'MLPtest=MLP(rng=numpy.random.RandomState(1234),input=numpy.ndarray([2,3]),n_in=2, n_hidden=3, n_out=4)'
tp200
a(g25
I89
g26
S'+ (self.logRegressionLayer.W ** 2).sum()'
tp201
aasbsg28
Nsg29
g1
(g30
g3
NtRp202
(dp203
g33
(lp204
g1
(g7
g3
NtRp205
(dp206
g10
S'auto_51'
p207
sg12
I0
sg13
(itheano.gof.utils
scratchpad
p208
(dp209
g16
(lp210
(lp211
(g19
I110
g20
S'test()'
tp212
a(g22
I105
g23
S'MLPtest=MLP(rng=numpy.random.RandomState(1234),input=numpy.ndarray([2,3]),n_in=2, n_hidden=3, n_out=4)'
tp213
a(g25
I89
g26
S'+ (self.logRegressionLayer.W ** 2).sum()'
tp214
aasbsg28
Nsg29
g1
(g30
g3
NtRp215
(dp216
g33
(lp217
g1
(g61
g3
NtRp218
(dp219
g10
S'auto_15'
p220
sg12
Nsg13
(itheano.gof.utils
scratchpad
p221
(dp222
g16
(lp223
(lp224
(g19
I110
g20
S'test()'
tp225
a(g22
I105
g23
S'MLPtest=MLP(rng=numpy.random.RandomState(1234),input=numpy.ndarray([2,3]),n_in=2, n_hidden=3, n_out=4)'
tp226
a(g25
I79
g26
S'n_out=n_out'
tp227
a(S'/space/garbersc/MNIST-tut/loregTut.py'
p228
I85
g26
S'borrow=True'
tp229
aasbsg74
g1
(g75
g3
NtRp230
(dp231
g28
S'W'
sg78
(lp232
g80
(g81
(I0
tS'b'
tRp233
(I1
(I3
I4
tg84
I00
S'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00'
tbasg85
I00
sg86
I00
sg87
g1
(g88
g3
NtRp234
(dp235
g91
(I00
I00
tp236
sg93
S'float64'
p237
sg95
g84
sg96
I00
sg28
Nsbsg97
Nsbsg28
S'W'
sg29
Nsg87
g234
sbag1
(g7
g3
NtRp238
(dp239
g10
S'auto_50'
p240
sg12
I0
sg13
(itheano.gof.utils
scratchpad
p241
(dp242
g16
(lp243
(lp244
(g19
I110
g20
S'test()'
tp245
a(g22
I105
g23
S'MLPtest=MLP(rng=numpy.random.RandomState(1234),input=numpy.ndarray([2,3]),n_in=2, n_hidden=3, n_out=4)'
tp246
a(g25
I89
g26
S'+ (self.logRegressionLayer.W ** 2).sum()'
tp247
aasbsg28
Nsg29
g1
(g30
g3
NtRp248
(dp249
g33
(lp250
g112
asg13
(itheano.gof.utils
scratchpad
p251
(dp252
bsg127
(lp253
g238
asg129
g1
(g130
g3
NtRp254
(dp255
g133
(lp256
sg135
(lp257
sg137
(lp258
I0
aI1
asg139
(tsg140
I00
sg141
(S'x'
S'x'
tp259
sg143
g144
sbsbsg87
g1
(g88
g3
NtRp260
(dp261
g91
(I01
I01
tp262
sg93
g121
sg95
g122
sg96
I00
sg28
Nsbsbasg13
(itheano.gof.utils
scratchpad
p263
(dp264
bsg127
(lp265
g205
asg129
g152
sbsg87
g1
(g88
g3
NtRp266
(dp267
g91
(I00
I00
tp268
sg93
g177
sg95
g84
sg96
I00
sg28
Nsbsbasg13
(itheano.gof.utils
scratchpad
p269
(dp270
bsg127
(lp271
g192
asg129
g1
(g181
g3
NtRp272
(dp273
g184
g177
sg93
g177
sg143
g144
sg156
g186
sg189
Nsbsbsg87
g1
(g88
g3
NtRp274
(dp275
g91
(tsg93
g177
sg95
g84
sg96
I00
sg28
Nsbsbasg13
(itheano.gof.utils
scratchpad
p276
(dp277
bsg127
(lp278
g8
asg129
g1
(g151
g3
NtRp279
(dp280
g154
g155
sg156
g186
sg28
S'Elemwise{add,no_inplace}'
p281
sg143
g144
sg164
(dp282
sg166
(S'add'
I2
I1
tp283
sg169
(dp284
sg171
I00
sg172
S"elementwise addition\n\n    Generalizes a scalar op to tensors.\n\n    All the inputs must have the same number of dimensions. When the\n    Op is performed, for each dimension, each input's size for that\n    dimension must be the same. As a special case, it can also be 1\n    but only if the input's broadcastable flag is True for that\n    dimension. In that case, the tensor is (virtually) replicated\n    along that dimension to match the size of the others.\n\n    The dtypes of the outputs mirror those of the scalar Op that is\n    being generalized to tensors. In particular, if the calculations\n    for an output are done inplace on an input, the output type must\n    be the same as the corresponding input type (see the doc of\n    scalar.ScalarOp to get help about controlling the output type)\n\n    Parameters\n    ----------\n    scalar_op\n        An instance of a subclass of scalar.ScalarOp which works uniquely\n        on scalars.\n    inplace_pattern\n        A dictionary that maps the index of an output to the\n        index of an input so the output is calculated inplace using\n        the input's storage. (Just like destroymap, but without the lists.)\n    nfunc_spec\n        Either None or a tuple of three elements,\n        (nfunc_name, nin, nout) such that getattr(numpy, nfunc_name)\n        implements this operation, takes nin inputs and nout outputs.\n        Note that nin cannot always be inferred from the scalar op's\n        own nin field because that value is sometimes 0 (meaning a\n        variable number of inputs), whereas the numpy function may\n        not have varargs.\n\n    Examples\n    --------\n    Elemwise(add) # represents + on tensors (x + y)\n    Elemwise(add, {0 : 0}) # represents the += operation (x += y)\n    Elemwise(add, {0 : 1}) # represents += on the second argument (y += x)\n    Elemwise(mul)(rand(10, 5), rand(1, 5)) # the second input is completed\n    # along the first dimension to match the first input\n    Elemwise(true_div)(rand(10, 5), rand(10, 1)) # same but along the\n    # second dimension\n    Elemwise(int_div)(rand(1, 5), rand(10, 1)) # the output has size (10, 5)\n    Elemwise(log)(rand(3, 4, 5))\n\n    "
p285
sbsbsg87
g1
(g88
g3
NtRp286
(dp287
g91
(tsg93
g177
sg95
g84
sg96
I00
sg28
NsbsbsS'negative_log_likelihood'
p288
